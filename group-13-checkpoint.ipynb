{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Residential Real Estate Sale Price in Polk County Based On Unit Features**\n",
    "## *Data Programming with Python - Group 13 Project*\n",
    "### *Sean O'Bryan, Navin Mukraj, Eric Schelin*\n",
    "\n",
    "<hr size=8>\n",
    "\n",
    "<img align=\"right\" src=\"https://www.python.org/static/community_logos/python-logo.png\">\n",
    "<img align=\"left\" src=\"https://www.dsm.city/_assets_/images/logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input variable number of features and their attributes and output expected sale price.  (or price per square foot-10/6)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data - done\n",
    "# clean the nulls from all data - done\n",
    "# add sale year to data - done\n",
    "# change yyyy dates to int64n - done\n",
    "# create the adjusted price - done\n",
    "# add adjusted price per square foot - done \n",
    "# add coords to data - \n",
    "# reset the index\n",
    "# correlation matrix\n",
    "# take columns with correlation > .5 for sale price\n",
    "# take columns with correlation > .5 for adjust price per square foot\n",
    "# create models for each situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# http://web.assess.co.polk.ia.us/web/exports/resA/sales/2020.txt\n",
    "# download all the files for all POLK\n",
    "# takes about 1min30secs\n",
    "import wget\n",
    "import os\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "def download_polk_res_sales(overwrite=False):\n",
    "    if overwrite:\n",
    "        min_year = 1990\n",
    "        max_year = date.today().year\n",
    "        # base url with the year as a placeholder\n",
    "        # base_url = \"http://web.assess.co.polk.ia.us/web/exports/res/sales/{}/DM.txt\"\n",
    "        base_url = \"http://web.assess.co.polk.ia.us/web/exports/resA/sales/{}.txt\"\n",
    "        # base name\n",
    "        base_name = \"POLK-{}.csv\"\n",
    "\n",
    "        for year in range(min_year,max_year+1):\n",
    "            url = base_url.format(year)\n",
    "            name = base_name.format(year)\n",
    "            path = \"./data/{}\".format(name)\n",
    "            if os.path.exists(path):\n",
    "                os.remove(path) \n",
    "            wget.download(url,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read real estate from the downloaded files\n",
    "# download the files first\n",
    "# this takes about 1.5secs\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "def import_all_polk_res_sales_from_csv():\n",
    "    min_year = 1990\n",
    "    max_year = date.today().year\n",
    "    # base url with the year as a placeholder\n",
    "    base_url = \"./data/POLK-{}.csv\"\n",
    "\n",
    "    # list of csv location by year\n",
    "    urls = list(map(lambda x: base_url.format(x), range(min_year,max_year+1)))\n",
    "    # this took 1min 30s\n",
    "    # read in all the tab delimited csv's into a dataframe\n",
    "    df_import = pd.concat((pd.read_csv(url, sep=\"\\t\") for url in urls))\n",
    "\n",
    "    df_import.shape\n",
    "    return df_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all_polk_res_sales_to_csv(df_to_export,overwrite=False,export_location=\"./data/POLK-all-years.csv\"):\n",
    "    if overwrite:\n",
    "#         # export the whole dataframe to a combined csv\n",
    "#         export_location = \"./data/POLK-all-years.csv\"\n",
    "        # delete if exists\n",
    "        if os.path.exists(export_location):\n",
    "            os.remove(export_location) \n",
    "        # export to csv\n",
    "        df_to_export.to_csv(export_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DtypeWarning: Columns (74) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "C:\\Users\\user2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DtypeWarning: Columns (67,74) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "C:\\Users\\user2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DtypeWarning: Columns (67) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "download_polk_res_sales()\n",
    "df = import_all_polk_res_sales_from_csv()\n",
    "export_all_polk_res_sales_to_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# clean all the data\n",
    "# Given that we will do multi-variable regression, remove any incomplete observation\n",
    "# df.dropna(inplace=True)\n",
    "df.dropna(subset=['gp'], inplace=True)\n",
    "df.gp = df.gp.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6       802423130005\n",
       "7       802423106016\n",
       "8       802423427003\n",
       "9       802425351001\n",
       "10      802413101008\n",
       "            ...     \n",
       "9707    792536102015\n",
       "9708    782502152020\n",
       "9709    782502152008\n",
       "9710    782502151018\n",
       "9711    782502152006\n",
       "Name: gp, Length: 336202, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.isnull().sum()\n",
    "df[\"gp\"]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sale year to data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with four digit year\n",
    "df['sale_date'] =  pd.to_datetime(df['sale_date'])\n",
    "df['sale_year'] = pd.DatetimeIndex(df['sale_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# convert year_built to int\n",
    "df.dropna(subset=['year_built'], inplace=True)\n",
    "df[\"year_built\"] = df[\"year_built\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the adjusted price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 288512\n",
      "2020: 19046\n",
      "need adj: 269466\n",
      "sum of 2020 and need adj: 288512\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# split data that needs to be adjusted for cpi\n",
    "# with no nulls - adjust for cpi\n",
    "df_2020 = df[df.sale_year > 2018]\n",
    "df_need_adj = df[df.sale_year < 2019]\n",
    "print(\"Total: {}\".format(df.shape[0]))\n",
    "print(\"2020: {}\".format(df_2020.shape[0]))\n",
    "print(\"need adj: {}\".format(df_need_adj.shape[0]))\n",
    "print(\"sum of 2020 and need adj: {}\".format((df_need_adj.shape[0]+df_2020.shape[0]+df['year_built'].isnull().sum())))\n",
    "\n",
    "print(df['year_built'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cpi in c:\\users\\user2\\anaconda3\\lib\\site-packages (0.1.16)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from cpi) (2.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from cpi) (2.8.0)\n",
      "Requirement already satisfied: pandas>=0.23.1 in c:\\users\\user2\\appdata\\roaming\\python\\python37\\site-packages (from cpi) (1.1.1)\n",
      "Requirement already satisfied: click>=6.7 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from cpi) (7.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests>=2.20.0->cpi) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests>=2.20.0->cpi) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests>=2.20.0->cpi) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests>=2.20.0->cpi) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->cpi) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from pandas>=0.23.1->cpi) (1.19.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from pandas>=0.23.1->cpi) (2019.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user2\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install cpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user2\\Anaconda3\\lib\\site-packages\\cpi\\__init__.py:46: StaleDataWarning: CPI data is out of date. To accurately inflate to today's dollars, you must run `cpi.update()`.\n",
      "  warnings.warn(StaleDataWarning())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function cpi.update()>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install, import, and update cpi library\n",
    "import cpi\n",
    "cpi.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_col(data, price, year):\n",
    "    return data.apply(lambda x: cpi.inflate(x[price], \n",
    "                      x[year]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_column(data, price, year):\n",
    "    return data.apply(lambda x: cpi.inflate(x[price], \n",
    "                      x[year]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# adjust the sale price column \"price\" for inflation based on the column \"sale_year\" \n",
    "\n",
    "df_need_adj[\"price_adj\"] = inflate_column(df_need_adj,'price','sale_year')\n",
    "# df_need_adj[\"price_adj\"] = df_need_adj.apply(lambda x: cpi.inflate(x.price,x.sale_year),axis=1)\n",
    "# df_need_adj.to_csv(\"./data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(df_need_adj[\"price_adj\"].dtype)\n",
    "\n",
    "df_need_adj[\"price_adj\"] = df_need_adj[\"price_adj\"].astype(np.int64)\n",
    "\n",
    "print(df_need_adj[\"price_adj\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19046\n",
      "19046\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# combine and fill in the 2019,2020 values with non-adjusted price\n",
    "df4 = pd.concat([df_need_adj,df_2020])\n",
    "# df4.to_csv(\"./data/test.csv\")\n",
    "print(df4['price_adj'].isnull().sum())\n",
    "print(df_2020.shape[0])\n",
    "# df4['price_adj'].fillna(df4['price'])\n",
    "df4.price_adj = df4.price_adj.fillna(value=df4.price)\n",
    "print(df4['price_adj'].isnull().sum())\n",
    "# finally replace our working object with the transformed object\n",
    "df = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate square foot\n",
    "# adjusted price / total living area \n",
    "\n",
    "# adjusted_psf\n",
    "df['adjusted_psf'] = df.price_adj/df.total_living_area\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_all_polk_res_sales_to_csv(df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urlextract in c:\\users\\user2\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\user2\\anaconda3\\lib\\site-packages (from urlextract) (1.4.4)\n",
      "Requirement already satisfied: idna in c:\\users\\user2\\anaconda3\\lib\\site-packages (from urlextract) (2.8)\n",
      "Requirement already satisfied: uritools in c:\\users\\user2\\anaconda3\\lib\\site-packages (from urlextract) (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user2\\anaconda3\\lib\\site-packages (from urlextract) (3.0.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user2\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user2\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from requests) (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user2\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: furl in c:\\users\\user2\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from furl) (1.0.1)\n",
      "Requirement already satisfied: six>=1.8.0 in c:\\users\\user2\\anaconda3\\lib\\site-packages (from furl) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user2\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in c:\\users\\user2\\anaconda3\\lib\\site-packages (2.5.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user2\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# extract coords - not used yet\n",
    "!pip install urlextract\n",
    "!pip install requests\n",
    "!pip install furl\n",
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from urlextract import URLExtract\n",
    "import requests\n",
    "from furl import furl\n",
    "import json\n",
    "\n",
    "def convertAddressToCoords(address=\"5106 OVID AVE, des Moines, IA 50310\"):\n",
    "    try:\n",
    "        f = furl('http://localhost:4000/v1/search')\n",
    "        f.path = 'v1/search'\n",
    "        f.args['text'] = address\n",
    "#         print(f.url)\n",
    "        # request geoparcel url\n",
    "        req = requests.get(f.url)\n",
    "        text = req.text\n",
    "#         print(text)\n",
    "        response_dict = json.loads(text)\n",
    "#         print(response_dict)\n",
    "#         print(response_dict[\"geocoding\"][\"query\"][\"focus.point.lat\"])\n",
    "#         print(response_dict[\"geocoding\"][\"query\"][\"focus.point.lon\"])\n",
    "        geodata = dict()\n",
    "        geodata['lat'] = response_dict[\"geocoding\"][\"query\"][\"focus.point.lat\"]\n",
    "        geodata['lng'] = response_dict[\"geocoding\"][\"query\"][\"focus.point.lon\"]\n",
    "        return geodata\n",
    "    except:\n",
    "        print(\"problem getting the coords for: {}\".format(address))\n",
    "        geodata = dict()\n",
    "        return geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to apply address conversion to a dataframe\n",
    "def convertAddressToCoords_col(data, address_col):\n",
    "    return data[address_col].apply(lambda x: convertAddressToCoords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jurisdiction</th>\n",
       "      <th>nbhd</th>\n",
       "      <th>dp</th>\n",
       "      <th>gp</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>book</th>\n",
       "      <th>pg</th>\n",
       "      <th>instrument</th>\n",
       "      <th>price</th>\n",
       "      <th>address</th>\n",
       "      <th>...</th>\n",
       "      <th>platname</th>\n",
       "      <th>begin_of_legal</th>\n",
       "      <th>school_district</th>\n",
       "      <th>fin_bsmt_area1</th>\n",
       "      <th>fin_bsmt_qual1</th>\n",
       "      <th>fin_bsmt_area2</th>\n",
       "      <th>fin_bsmt_qual2</th>\n",
       "      <th>sale_year</th>\n",
       "      <th>price_adj</th>\n",
       "      <th>adjusted_psf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANKENY</td>\n",
       "      <td>AK02</td>\n",
       "      <td>18100072001000</td>\n",
       "      <td>802423106016</td>\n",
       "      <td>1990-09-26</td>\n",
       "      <td>6307</td>\n",
       "      <td>786</td>\n",
       "      <td>Deed</td>\n",
       "      <td>38900</td>\n",
       "      <td>1133 SW 3RD ST</td>\n",
       "      <td>...</td>\n",
       "      <td>SEC 23-80-24</td>\n",
       "      <td>BEG 1292.25F W &amp; 481.5F</td>\n",
       "      <td>Ankeny</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>74736.0</td>\n",
       "      <td>54.831988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANKENY</td>\n",
       "      <td>AK02</td>\n",
       "      <td>18100097024019</td>\n",
       "      <td>802425351001</td>\n",
       "      <td>1990-06-15</td>\n",
       "      <td>6255</td>\n",
       "      <td>306</td>\n",
       "      <td>Contract</td>\n",
       "      <td>200000</td>\n",
       "      <td>7983 NE 14TH ST</td>\n",
       "      <td>...</td>\n",
       "      <td>SEC 25-80-24</td>\n",
       "      <td>-EX W OF LN BEG 115.4F E</td>\n",
       "      <td>Ankeny</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>384249.0</td>\n",
       "      <td>197.862513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ANKENY</td>\n",
       "      <td>AK02</td>\n",
       "      <td>18100111000000</td>\n",
       "      <td>802423203001</td>\n",
       "      <td>1990-10-17</td>\n",
       "      <td>6298</td>\n",
       "      <td>916</td>\n",
       "      <td>Contract</td>\n",
       "      <td>43200</td>\n",
       "      <td>101 SW CHERRY ST</td>\n",
       "      <td>...</td>\n",
       "      <td>TOWN OF ANKENY</td>\n",
       "      <td>LOT 10 BLK 1</td>\n",
       "      <td>Ankeny</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>82997.0</td>\n",
       "      <td>55.927898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANKENY</td>\n",
       "      <td>AK02</td>\n",
       "      <td>18100121000000</td>\n",
       "      <td>802423202001</td>\n",
       "      <td>1990-06-13</td>\n",
       "      <td>6248</td>\n",
       "      <td>716</td>\n",
       "      <td>Deed</td>\n",
       "      <td>41000</td>\n",
       "      <td>101 SW MAPLE ST</td>\n",
       "      <td>...</td>\n",
       "      <td>TOWN OF ANKENY</td>\n",
       "      <td>LOT 10 BLK 2</td>\n",
       "      <td>Ankeny</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>78771.0</td>\n",
       "      <td>117.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANKENY</td>\n",
       "      <td>AK02</td>\n",
       "      <td>18100209011000</td>\n",
       "      <td>802423254009</td>\n",
       "      <td>1990-09-28</td>\n",
       "      <td>6291</td>\n",
       "      <td>819</td>\n",
       "      <td>Deed</td>\n",
       "      <td>60000</td>\n",
       "      <td>605 SW MAPLE ST</td>\n",
       "      <td>...</td>\n",
       "      <td>ANKENY VILLAGE PLAT 2</td>\n",
       "      <td>LOT 11</td>\n",
       "      <td>Ankeny</td>\n",
       "      <td>548.0</td>\n",
       "      <td>Average Plus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990</td>\n",
       "      <td>115274.0</td>\n",
       "      <td>123.155983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   jurisdiction  nbhd              dp            gp  sale_date  book   pg  \\\n",
       "7        ANKENY  AK02  18100072001000  802423106016 1990-09-26  6307  786   \n",
       "9        ANKENY  AK02  18100097024019  802425351001 1990-06-15  6255  306   \n",
       "11       ANKENY  AK02  18100111000000  802423203001 1990-10-17  6298  916   \n",
       "12       ANKENY  AK02  18100121000000  802423202001 1990-06-13  6248  716   \n",
       "13       ANKENY  AK02  18100209011000  802423254009 1990-09-28  6291  819   \n",
       "\n",
       "   instrument   price           address  ...               platname  \\\n",
       "7        Deed   38900    1133 SW 3RD ST  ...           SEC 23-80-24   \n",
       "9    Contract  200000   7983 NE 14TH ST  ...           SEC 25-80-24   \n",
       "11   Contract   43200  101 SW CHERRY ST  ...         TOWN OF ANKENY   \n",
       "12       Deed   41000   101 SW MAPLE ST  ...         TOWN OF ANKENY   \n",
       "13       Deed   60000   605 SW MAPLE ST  ...  ANKENY VILLAGE PLAT 2   \n",
       "\n",
       "              begin_of_legal school_district  fin_bsmt_area1  fin_bsmt_qual1  \\\n",
       "7    BEG 1292.25F W & 481.5F          Ankeny             0.0             NaN   \n",
       "9   -EX W OF LN BEG 115.4F E          Ankeny             0.0             NaN   \n",
       "11              LOT 10 BLK 1          Ankeny             0.0             NaN   \n",
       "12              LOT 10 BLK 2          Ankeny             0.0             NaN   \n",
       "13                    LOT 11          Ankeny           548.0    Average Plus   \n",
       "\n",
       "    fin_bsmt_area2 fin_bsmt_qual2 sale_year price_adj  adjusted_psf  \n",
       "7              0.0            NaN      1990   74736.0     54.831988  \n",
       "9              0.0            NaN      1990  384249.0    197.862513  \n",
       "11             0.0            NaN      1990   82997.0     55.927898  \n",
       "12             0.0            NaN      1990   78771.0    117.218750  \n",
       "13             0.0            NaN      1990  115274.0    123.155983  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "ddata = dd.from_pandas(df, npartitions=30)\n",
    "df_small[\"coords\"] = ddata.map_partitions(convertGeoparcelToCoords_col(df_small, 'gp')).compute(get=get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "{'lat': 45.52, 'lng': -122.67}\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for iter in range(100):\n",
    "    print(getCoords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coords - not used yet\n",
    "\n",
    "from urlextract import URLExtract\n",
    "import requests\n",
    "from furl import furl\n",
    "\n",
    "# geoparcel url (gp)\n",
    "url = \"http://web.assess.co.polk.ia.us/cgi-bin/web/maps/googlemap/04000614005000\"\n",
    "#      http://web.assess.co.polk.ia.us/cgi-bin/web/maps/googlemap/792430177024\n",
    "\n",
    "# request geoparcel url\n",
    "req = requests.get(url)\n",
    "text = req.text\n",
    "print(\"Response is: {}\\n\".format(text))\n",
    "\n",
    "# extract url from redirect response\n",
    "extractor = URLExtract()\n",
    "urls = extractor.find_urls(text)\n",
    "print(urls[0]) # prints: ['example.com']\n",
    "\n",
    "# get raw coords from url\n",
    "f = furl(urls[0]) \n",
    "coords = f.args['q']\n",
    "print(coords)\n",
    "\n",
    "geodata = dict()\n",
    "geodata['lat'] = coords.split(',')[0]\n",
    "geodata['lng'] = coords.split(',')[1]\n",
    "print(geodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from urlextract import URLExtract\n",
    "import requests\n",
    "from furl import furl\n",
    "\n",
    "def convertGeoparcelToCoords(geoparcel):\n",
    "    try:\n",
    "        # geoparcel url (gp)\n",
    "        print(\"Getting coords for gp={}\".format(geoparcel))\n",
    "        url = \"http://web.assess.co.polk.ia.us/cgi-bin/web/maps/googlemap/{}\".format(geoparcel)\n",
    "\n",
    "        # request geoparcel url\n",
    "        req = requests.get(url)\n",
    "        text = req.text\n",
    "\n",
    "        # extract url from redirect response\n",
    "        extractor = URLExtract()\n",
    "        urls = extractor.find_urls(text)\n",
    "        print(urls[0]) # prints: ['example.com']\n",
    "\n",
    "        # get raw coords from url\n",
    "        f = furl(urls[0]) \n",
    "        coords = f.args['q']\n",
    "\n",
    "        geodata = dict()\n",
    "        geodata['lat'] = coords.split(',')[0]\n",
    "        geodata['lng'] = coords.split(',')[1]\n",
    "        return geodata\n",
    "    except:\n",
    "        print(\"problem getting the coords for: {}\".format(geoparcel))\n",
    "        geodata = dict()\n",
    "        return geodata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to apply geoparcel conversion to a dataframe\n",
    "def convertGeoparcelToCoords_col(data, gp_col):\n",
    "    return data[gp_col].apply(lambda x: convertGeoparcelToCoords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "ddata = dd.from_pandas(df, npartitions=30)\n",
    "df[\"coords\"] = ddata.map_partitions(convertGeoparcelToCoords_col(df, 'gp')).compute(get=get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"coords\"] = convertGeoparcelToCoords_col(df,'gp')\n",
    "# df_need_adj[\"price_adj\"] = df_need_adj.apply(lambda x: cpi.inflate(x.price,x.sale_year),axis=1)\n",
    "# df.gp = df.gp.astype(np.int64)\n",
    "df[\"coords\"] = df[\"gp\"].apply(lambda x: convertGeoparcelToCoords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "782416351039\n",
    "782422101050\n",
    "\n",
    "792430177024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(convertGeoparcelToCoords(\"04000614005000\"))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on correlation matrix, copy highly correlated features\n",
    "df2 = df[['price','land_full','bldg_full','total_full','main_living_area',\n",
    "          'total_living_area','basement_area','att_garage_area','bathrooms',\n",
    "          'extra_fixtures','fireplaces','year_built','sale_date']].copy()\n",
    "pd.set_option('display.max_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that we will do multi-variable regression, remove any incomplete observation\n",
    "df2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with four digit year\n",
    "df2['sale_date'] =  pd.to_datetime(df2['sale_date'])\n",
    "df2['sale_year'] = pd.DatetimeIndex(df2['sale_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# convert year_built to int\n",
    "df2[\"year_built\"] = df2[\"year_built\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our dataframe\n",
    "print(df2.isna().sum())\n",
    "print(df2.count())\n",
    "print(df2.shape)\n",
    "df2.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index\n",
    "df2 = df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data that needs to be adjusted for cpi\n",
    "# with no nulls - adjust for cpi\n",
    "df_2020 = df2[df2.sale_year > 2018]\n",
    "df_need_adj = df2[df2.sale_year < 2019]\n",
    "print(\"Total: {}\".format(df2.shape[0]))\n",
    "print(\"2020: {}\".format(df_2020.shape[0]))\n",
    "print(\"need adj: {}\".format(df_need_adj.shape[0]))\n",
    "print(\"sum of 2020 and need adj: {}\".format((df_need_adj.shape[0]+df_2020.shape[0]+df2['year_built'].isnull().sum())))\n",
    "\n",
    "print(df2['year_built'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# http GET localhost:8080/appname/functionname?param1=something&param2=something2\n",
    "\n",
    "# return predicted-price: '123.02'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install, import, and update cpi library\n",
    "!pip install cpi\n",
    "import cpi\n",
    "cpi.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_col(data, price, year):\n",
    "    return data.apply(lambda x: cpi.inflate(x[price], \n",
    "                      x[year]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the sale price column \"price\" for inflation based on the column \"sale_year\" \n",
    "\n",
    "df_need_adj[\"price_adj\"] = inflate_column(df_need_adj,'price','sale_year')\n",
    "# df_need_adj[\"price_adj\"] = df_need_adj.apply(lambda x: cpi.inflate(x.price,x.sale_year),axis=1)\n",
    "# df_need_adj.to_csv(\"./data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the adjusted price from float64 to int64 (match raw sale price column)\n",
    "df_need_adj[\"price_adj\"] = df_need_adj[\"price_adj\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes\n",
    "df_adjusted_temp = pd.concat([df_need_adj,df_2020])\n",
    "# df_adjusted_temp.to_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the 2019,2020 values with non-adjusted price\n",
    "print(\"Total NA's before filling: {}\".format(df_adjusted_temp['price_adj'].isnull().sum()))\n",
    "# df4['price_adj'].fillna(df4['price']) - don't work\n",
    "df_adjusted_temp.price_adj = df_adjusted_temp.price_adj.fillna(value=df_adjusted_temp.price)\n",
    "print(\"Total NA's after filling: {}\".format(df_adjusted_temp['price_adj'].isnull().sum()))\n",
    "# finally replace our working object with the transformed object\n",
    "df2 = df_adjusted_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr().style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df2.hist(bins=50, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df_yearblt = df2.year_built\n",
    "\n",
    "df_yearblt.hist(bins=50, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=train_test_split(df2,train_size=0.8,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = ['bathrooms','year_built','total_living_area','att_garage_area','extra_fixtures','fireplaces','basement_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyfeat=PolynomialFeatures(degree=3)\n",
    "xtrain_poly=polyfeat.fit_transform(train_data[features2])\n",
    "xtest_poly=polyfeat.fit_transform(test_data[features2])\n",
    "\n",
    "poly=linear_model.LinearRegression()\n",
    "poly.fit(xtrain_poly,train_data['price_adj'])\n",
    "polypred=poly.predict(xtest_poly)\n",
    "\n",
    "print('complex model')\n",
    "mean_squared_error=metrics.mean_squared_error(test_data['price_adj'],polypred)\n",
    "print('Mean Squared Error (MSE) ', round(np.sqrt(mean_squared_error), 2))\n",
    "print('R-squared (training) ', round(poly.score(xtrain_poly, train_data['price_adj']), 3))\n",
    "print('R-squared (testing) ', round(poly.score(xtest_poly, test_data['price_adj']), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
